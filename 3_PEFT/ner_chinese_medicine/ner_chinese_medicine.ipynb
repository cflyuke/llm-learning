{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bda2f63a",
   "metadata": {},
   "source": [
    "## 模型加载\n",
    "### QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2309172f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 17:17:30,997 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 17:17:44,790 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a58a132dfda4aa587b4ea0ab1a8d82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from modelscope import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model_name_or_path = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True, force_download=True, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "# model.enable_input_require_grads() ## set it if use gradient checkpointing to save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0577b44",
   "metadata": {},
   "source": [
    "## 数据集转换及加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fff18e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def transfer_dataset(origin_path, new_path): \n",
    "    with open(origin_path, \"r\") as f:\n",
    "        messages = []\n",
    "        contents = [s.strip() for s in f.read().split('\\n\\n')]\n",
    "        for content in contents:\n",
    "            parts = content.split('\\n')\n",
    "            input_text = \"\"\n",
    "            labels_list = []\n",
    "            for part in parts:\n",
    "                input_text += part[0]\n",
    "                if part[2] == 'B':\n",
    "                    labels_list.append({\n",
    "                        \"entity_name\": part[0],\n",
    "                        \"entity_label\": part[4:],\n",
    "                    })\n",
    "                elif part[2] == \"I\":\n",
    "                    labels_list[-1][\"entity_name\"] += part[0]\n",
    "            message = {\n",
    "                \"text\": input_text,\n",
    "                \"labels\": json.dumps(labels_list, ensure_ascii=False)  # 用空格连接标签\n",
    "            }\n",
    "            messages.append(message)\n",
    "    with open(new_path, \"w\") as f:\n",
    "        for message in messages:\n",
    "            f.write(json.dumps(message, ensure_ascii=False) + '\\n')\n",
    "\n",
    "origin_paths = [\"medical.dev\", \"medical.test\", \"medical.train\"]\n",
    "new_paths = [\"eval.jsonl\", \"test.jsonl\", \"train.jsonl\"]\n",
    "data_dir = \"data\"\n",
    "for origin_path, new_path in zip(origin_paths, new_paths):\n",
    "    transfer_dataset(os.path.join(data_dir, origin_path), os.path.join(data_dir, new_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84278c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592dea5b3aa04a2d91448ea0a2d4602f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c5f47895e04e9d982b1381a483d1ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating eval split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'labels'],\n",
      "        num_rows: 5259\n",
      "    })\n",
      "    eval: Dataset({\n",
      "        features: ['text', 'labels'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "train_file = os.path.join(data_dir, \"train.jsonl\")\n",
    "eval_file = os.path.join(data_dir, \"eval.jsonl\")\n",
    "test_file = os.path.join(data_dir, \"test.jsonl\")\n",
    "data_files = {\n",
    "    \"train\": train_file,\n",
    "    \"eval\": eval_file,\n",
    "}\n",
    "dataset_dict = load_dataset(\"json\", data_files=data_files)\n",
    "dataset_dict[\"eval\"] = dataset_dict[\"eval\"].shuffle(seed=42).select(range(100))\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f1ebd35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a82e25db259414bbc0a4c8fd1e0c76a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5259 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805a39d892724095b708cf07c4322801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"你是一个中医药领域的专家，你需要从给定的句子中提取实体信息。所有的实体种类: 中医治则 中医治疗 中医证候 中医诊断 中药 临床表现 其他治疗 方剂 西医治疗 西医诊断。\n",
    "每一个实体对应一个json格式，共同组成一个json列表，例如\"[{\"entity_name\": \"口苦\", \"entity_label\": \"临床表现\"}]\". \"\"\"\n",
    "MAX_LENGTH = 512\n",
    "LABELS = [\"中医治则\", \"中医治疗\", \"中医证候\", \"中医诊断\", \"中药\", \"临床表现\", \"其他治疗\", \"方剂\", \"西医治疗\", \"西医诊断\"]\n",
    "def preprocess_function(example):\n",
    "    model_inputs = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": example[\"text\"]},\n",
    "        ],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer(model_inputs, add_special_tokens=False)\n",
    "    labels = tokenizer(example[\"labels\"] + tokenizer.eos_token, add_special_tokens=False)\n",
    "    input_ids = model_inputs[\"input_ids\"] + labels[\"input_ids\"]\n",
    "    attention_mask = model_inputs[\"attention_mask\"] + labels[\"attention_mask\"]\n",
    "    labels = [-100] * len(model_inputs[\"input_ids\"]) + labels[\"input_ids\"]\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "dataset_dict = dataset_dict.map(preprocess_function, remove_columns=dataset_dict[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8308fe2a",
   "metadata": {},
   "source": [
    "## 准备LoRA模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c388469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b77dde",
   "metadata": {},
   "source": [
    "## Train & Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92184387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1974' max='1974' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1974/1974 45:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>中医治则</th>\n",
       "      <th>中医治疗</th>\n",
       "      <th>中医证候</th>\n",
       "      <th>中医诊断</th>\n",
       "      <th>中药</th>\n",
       "      <th>临床表现</th>\n",
       "      <th>其他治疗</th>\n",
       "      <th>方剂</th>\n",
       "      <th>西医治疗</th>\n",
       "      <th>西医诊断</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.104100</td>\n",
       "      <td>0.115781</td>\n",
       "      <td>{'precision': '0.6667', 'recall': '0.3333', 'f1': '0.4444'}</td>\n",
       "      <td>{'precision': '0.5556', 'recall': '0.5000', 'f1': '0.5263'}</td>\n",
       "      <td>{'precision': '0.8333', 'recall': '0.5172', 'f1': '0.6383'}</td>\n",
       "      <td>{'precision': '1.0000', 'recall': '0.5000', 'f1': '0.6667'}</td>\n",
       "      <td>{'precision': '0.8269', 'recall': '0.7288', 'f1': '0.7748'}</td>\n",
       "      <td>{'precision': '0.7368', 'recall': '0.2456', 'f1': '0.3684'}</td>\n",
       "      <td>{'precision': '0.0000', 'recall': '0.0000', 'f1': '0.0000'}</td>\n",
       "      <td>{'precision': '0.6364', 'recall': '0.3500', 'f1': '0.4516'}</td>\n",
       "      <td>{'precision': '0.2500', 'recall': '0.1667', 'f1': '0.2000'}</td>\n",
       "      <td>{'precision': '0.7500', 'recall': '0.6316', 'f1': '0.6857'}</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.559701</td>\n",
       "      <td>0.528169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.093400</td>\n",
       "      <td>0.097643</td>\n",
       "      <td>{'precision': '1.0000', 'recall': '0.5000', 'f1': '0.6667'}</td>\n",
       "      <td>{'precision': '0.5556', 'recall': '0.5000', 'f1': '0.5263'}</td>\n",
       "      <td>{'precision': '0.7500', 'recall': '0.5172', 'f1': '0.6122'}</td>\n",
       "      <td>{'precision': '1.0000', 'recall': '0.5000', 'f1': '0.6667'}</td>\n",
       "      <td>{'precision': '0.8409', 'recall': '0.6271', 'f1': '0.7184'}</td>\n",
       "      <td>{'precision': '0.6000', 'recall': '0.2105', 'f1': '0.3117'}</td>\n",
       "      <td>{'precision': '0.0000', 'recall': '0.0000', 'f1': '0.0000'}</td>\n",
       "      <td>{'precision': '0.7500', 'recall': '0.4500', 'f1': '0.5625'}</td>\n",
       "      <td>{'precision': '0.0000', 'recall': '0.0000', 'f1': '0.0000'}</td>\n",
       "      <td>{'precision': '0.8485', 'recall': '0.7368', 'f1': '0.7887'}</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.550562</td>\n",
       "      <td>0.524064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.085100</td>\n",
       "      <td>0.079456</td>\n",
       "      <td>{'precision': '0.6667', 'recall': '0.3333', 'f1': '0.4444'}</td>\n",
       "      <td>{'precision': '0.8571', 'recall': '0.6000', 'f1': '0.7059'}</td>\n",
       "      <td>{'precision': '0.6471', 'recall': '0.3793', 'f1': '0.4783'}</td>\n",
       "      <td>{'precision': '1.0000', 'recall': '0.5000', 'f1': '0.6667'}</td>\n",
       "      <td>{'precision': '0.8261', 'recall': '0.6441', 'f1': '0.7238'}</td>\n",
       "      <td>{'precision': '0.7273', 'recall': '0.2807', 'f1': '0.4051'}</td>\n",
       "      <td>{'precision': '1.0000', 'recall': '0.5000', 'f1': '0.6667'}</td>\n",
       "      <td>{'precision': '0.6364', 'recall': '0.3500', 'f1': '0.4516'}</td>\n",
       "      <td>{'precision': '1.0000', 'recall': '0.3333', 'f1': '0.5000'}</td>\n",
       "      <td>{'precision': '0.7742', 'recall': '0.6316', 'f1': '0.6957'}</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.537879</td>\n",
       "      <td>0.518248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.069500</td>\n",
       "      <td>0.069358</td>\n",
       "      <td>{'precision': '0.0000', 'recall': '0.0000', 'f1': '0.0000'}</td>\n",
       "      <td>{'precision': '0.7143', 'recall': '0.5000', 'f1': '0.5882'}</td>\n",
       "      <td>{'precision': '0.7619', 'recall': '0.5517', 'f1': '0.6400'}</td>\n",
       "      <td>{'precision': '1.0000', 'recall': '0.5000', 'f1': '0.6667'}</td>\n",
       "      <td>{'precision': '0.8462', 'recall': '0.7458', 'f1': '0.7928'}</td>\n",
       "      <td>{'precision': '0.7576', 'recall': '0.4386', 'f1': '0.5556'}</td>\n",
       "      <td>{'precision': '0.0000', 'recall': '0.0000', 'f1': '0.0000'}</td>\n",
       "      <td>{'precision': '0.6364', 'recall': '0.3500', 'f1': '0.4516'}</td>\n",
       "      <td>{'precision': '0.6667', 'recall': '0.3333', 'f1': '0.4444'}</td>\n",
       "      <td>{'precision': '0.8438', 'recall': '0.7105', 'f1': '0.7714'}</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.617100</td>\n",
       "      <td>0.552413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.061900</td>\n",
       "      <td>0.074125</td>\n",
       "      <td>{'precision': '0.5000', 'recall': '0.3333', 'f1': '0.4000'}</td>\n",
       "      <td>{'precision': '0.6250', 'recall': '0.5000', 'f1': '0.5556'}</td>\n",
       "      <td>{'precision': '0.8095', 'recall': '0.5862', 'f1': '0.6800'}</td>\n",
       "      <td>{'precision': '1.0000', 'recall': '0.5000', 'f1': '0.6667'}</td>\n",
       "      <td>{'precision': '0.8269', 'recall': '0.7288', 'f1': '0.7748'}</td>\n",
       "      <td>{'precision': '0.7273', 'recall': '0.4211', 'f1': '0.5333'}</td>\n",
       "      <td>{'precision': '1.0000', 'recall': '1.0000', 'f1': '1.0000'}</td>\n",
       "      <td>{'precision': '0.7273', 'recall': '0.4000', 'f1': '0.5161'}</td>\n",
       "      <td>{'precision': '0.6667', 'recall': '0.3333', 'f1': '0.4444'}</td>\n",
       "      <td>{'precision': '0.7576', 'recall': '0.6579', 'f1': '0.7042'}</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.625926</td>\n",
       "      <td>0.555921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>0.072394</td>\n",
       "      <td>{'precision': '0.5000', 'recall': '0.3333', 'f1': '0.4000'}</td>\n",
       "      <td>{'precision': '0.7143', 'recall': '0.5000', 'f1': '0.5882'}</td>\n",
       "      <td>{'precision': '0.8636', 'recall': '0.6552', 'f1': '0.7451'}</td>\n",
       "      <td>{'precision': '1.0000', 'recall': '0.5000', 'f1': '0.6667'}</td>\n",
       "      <td>{'precision': '0.8600', 'recall': '0.7288', 'f1': '0.7890'}</td>\n",
       "      <td>{'precision': '0.6800', 'recall': '0.2982', 'f1': '0.4146'}</td>\n",
       "      <td>{'precision': '1.0000', 'recall': '0.5000', 'f1': '0.6667'}</td>\n",
       "      <td>{'precision': '0.7692', 'recall': '0.5000', 'f1': '0.6061'}</td>\n",
       "      <td>{'precision': '0.4000', 'recall': '0.3333', 'f1': '0.3636'}</td>\n",
       "      <td>{'precision': '0.8788', 'recall': '0.7632', 'f1': '0.8169'}</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.615970</td>\n",
       "      <td>0.551959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.054100</td>\n",
       "      <td>0.070908</td>\n",
       "      <td>{'precision': '0.2500', 'recall': '0.1667', 'f1': '0.2000'}</td>\n",
       "      <td>{'precision': '0.7143', 'recall': '0.5000', 'f1': '0.5882'}</td>\n",
       "      <td>{'precision': '0.7391', 'recall': '0.5862', 'f1': '0.6538'}</td>\n",
       "      <td>{'precision': '1.0000', 'recall': '0.5000', 'f1': '0.6667'}</td>\n",
       "      <td>{'precision': '0.8600', 'recall': '0.7288', 'f1': '0.7890'}</td>\n",
       "      <td>{'precision': '0.6400', 'recall': '0.2807', 'f1': '0.3902'}</td>\n",
       "      <td>{'precision': '1.0000', 'recall': '0.5000', 'f1': '0.6667'}</td>\n",
       "      <td>{'precision': '0.8750', 'recall': '0.7000', 'f1': '0.7778'}</td>\n",
       "      <td>{'precision': '0.3333', 'recall': '0.3333', 'f1': '0.3333'}</td>\n",
       "      <td>{'precision': '0.9167', 'recall': '0.8684', 'f1': '0.8919'}</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.636704</td>\n",
       "      <td>0.560132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>0.072242</td>\n",
       "      <td>{'precision': '0.5000', 'recall': '0.3333', 'f1': '0.4000'}</td>\n",
       "      <td>{'precision': '0.7500', 'recall': '0.6000', 'f1': '0.6667'}</td>\n",
       "      <td>{'precision': '0.7917', 'recall': '0.6552', 'f1': '0.7170'}</td>\n",
       "      <td>{'precision': '1.0000', 'recall': '0.5000', 'f1': '0.6667'}</td>\n",
       "      <td>{'precision': '0.8269', 'recall': '0.7288', 'f1': '0.7748'}</td>\n",
       "      <td>{'precision': '0.6923', 'recall': '0.3158', 'f1': '0.4337'}</td>\n",
       "      <td>{'precision': '1.0000', 'recall': '0.5000', 'f1': '0.6667'}</td>\n",
       "      <td>{'precision': '0.9286', 'recall': '0.6500', 'f1': '0.7647'}</td>\n",
       "      <td>{'precision': '0.5000', 'recall': '0.3333', 'f1': '0.4000'}</td>\n",
       "      <td>{'precision': '0.8571', 'recall': '0.7895', 'f1': '0.8219'}</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.641509</td>\n",
       "      <td>0.561983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.068128</td>\n",
       "      <td>{'precision': '0.5000', 'recall': '0.3333', 'f1': '0.4000'}</td>\n",
       "      <td>{'precision': '0.7500', 'recall': '0.6000', 'f1': '0.6667'}</td>\n",
       "      <td>{'precision': '0.8750', 'recall': '0.7241', 'f1': '0.7925'}</td>\n",
       "      <td>{'precision': '1.0000', 'recall': '0.5000', 'f1': '0.6667'}</td>\n",
       "      <td>{'precision': '0.8600', 'recall': '0.7288', 'f1': '0.7890'}</td>\n",
       "      <td>{'precision': '0.6923', 'recall': '0.3158', 'f1': '0.4337'}</td>\n",
       "      <td>{'precision': '0.5000', 'recall': '0.5000', 'f1': '0.5000'}</td>\n",
       "      <td>{'precision': '0.8571', 'recall': '0.6000', 'f1': '0.7059'}</td>\n",
       "      <td>{'precision': '0.6667', 'recall': '0.3333', 'f1': '0.4444'}</td>\n",
       "      <td>{'precision': '0.9167', 'recall': '0.8684', 'f1': '0.8919'}</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.565217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1974, training_loss=0.06665211873697051, metrics={'train_runtime': 2721.6049, 'train_samples_per_second': 5.797, 'train_steps_per_second': 0.725, 'total_flos': 1.342817220885719e+17, 'train_loss': 0.06665211873697051, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "from collections import defaultdict\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    logits = logits.argmax(axis=-1)\n",
    "    true_labels = [[label for label in seq if label != -100] for seq in labels]\n",
    "    true_predictions = [[p for (p, l) in zip(pred[:-1], lab[1:]) if l != -100] for pred, lab in zip(logits, labels)]\n",
    "    true_labels = tokenizer.batch_decode(true_labels, skip_special_tokens=True)\n",
    "    true_predictions = tokenizer.batch_decode(true_predictions, skip_special_tokens=True)\n",
    "    def parse_entities(text):\n",
    "        try:\n",
    "            entities = json.loads(text)\n",
    "            return set((entity[\"entity_name\"], entity[\"entity_label\"]) for entity in entities)\n",
    "        except:\n",
    "            return set()\n",
    "    true_labels = [parse_entities(text) for text in true_labels]\n",
    "    true_predictions = [parse_entities(text) for text in true_predictions]\n",
    "    category_metrics = defaultdict(lambda: {\"tp\": 0, \"fp\": 0, \"fn\": 0})\n",
    "    for labels, predictions in zip(true_labels, true_predictions):\n",
    "        pred_by_category = defaultdict(set)\n",
    "        label_by_category = defaultdict(set)\n",
    "        for name, label in labels:\n",
    "            if label in LABELS:\n",
    "                label_by_category[label].add(name)\n",
    "        for name, label in predictions:\n",
    "            if label in LABELS:\n",
    "                pred_by_category[label].add(name)\n",
    "        for category in LABELS:\n",
    "            label_set = label_by_category[category]\n",
    "            pred_set = pred_by_category[category]\n",
    "            category_metrics[category][\"tp\"] += len(label_set & pred_set)\n",
    "            category_metrics[category][\"fp\"] += len(pred_set - label_set)\n",
    "            category_metrics[category][\"fn\"] += len(label_set - pred_set)\n",
    "    overall_tp = overall_tp = overall_fn = 0\n",
    "    results = {}\n",
    "    for category, metrics in category_metrics.items():\n",
    "        tp, fp, fn = metrics[\"tp\"], metrics[\"fp\"], metrics[\"fn\"]\n",
    "        overall_tp += tp\n",
    "        overall_tp += fp\n",
    "        overall_fn += fn\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        results[category] = {\n",
    "            \"precision\": f\"{precision:.4f}\",\n",
    "            \"recall\": f\"{recall:.4f}\",\n",
    "            \"f1\": f\"{f1:.4f}\"\n",
    "        }\n",
    "    overall_precision = overall_tp / (overall_tp + overall_tp) if (overall_tp + overall_tp) > 0 else 0.0\n",
    "    overall_recall = overall_tp / (overall_tp + overall_fn) if (overall_tp + overall_fn) > 0 else 0.0\n",
    "    overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0.\n",
    "    results[\"overall_precision\"] = overall_precision\n",
    "    results[\"overall_recall\"] = overall_recall\n",
    "    results[\"overall_f1\"] = overall_f1\n",
    "    return results\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    eval_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"best\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    logging_steps=200,\n",
    "    save_total_limit=3,\n",
    "    metric_for_best_model=\"overall_f1\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "    eval_dataset=dataset_dict[\"eval\"],\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, padding=True),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff3c63a",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e9af262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa0eb00a21c4bf0b55ebad5ab70577e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 药进１０帖，黄疸稍退，饮食稍增，精神稍振\n",
      "label: [{\"entity_name\": \"黄疸\", \"entity_label\": \"中医诊断\"}]\n",
      "Prediction: system\n",
      "你是一个中医药领域的专家，你需要从给定的句子中提取实体信息。所有的实体种类: 中医治则 中医治疗 中医证候 中医诊断 中药 临床表现 其他治疗 方剂 西医治疗 西医诊断。\n",
      "每一个实体对应一个json格式，共同组成一个json列表，例如\"[{\"entity_name\": \"口苦\", \"entity_label\": \"临床表现\"}]\". \n",
      "user\n",
      "药进１０帖，黄疸稍退，饮食稍增，精神稍振\n",
      "assistant\n",
      "[{\"entity_name\": \"黄疸\", \"entity_label\": \"中医诊断\"}, {\"entity_name\": \"饮食稍增\", \"entity_label\": \"临床表现\"}, {\"entity_name\": \"精神稍振\", \"entity_label\": \"临床表现\"}]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_dataset = load_dataset(\"json\", data_files={\"test\": test_file})[\"test\"]\n",
    "with torch.no_grad():\n",
    "    example = test_dataset[0]\n",
    "    model_inputs = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": example[\"text\"]},\n",
    "        ],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer(model_inputs, return_tensors=\"pt\").to(model.device)\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=128)\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"Input: {example['text']}\")\n",
    "    print(f\"label: {example[\"labels\"]}\")\n",
    "    print(f\"Prediction: {generated_text}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
