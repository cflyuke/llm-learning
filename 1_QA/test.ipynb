{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc2bcd2c-4b58-4613-b096-3454be0cd5c7",
   "metadata": {},
   "source": [
    "## 生成式问答\n",
    "> pretrained_model: `mengzi-t5-base`\n",
    "\n",
    "> dataset: `DuReaderQG`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09852dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/root/autodl-tmp/wandb/offline-run-20250917_114007-xoadc3jl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import wandb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer, AutoModelForSeq2SeqLM\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from sacrebleu import BLEU\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "wandb.init(\n",
    "    project='llm-learning',\n",
    "    name='1_QA',\n",
    "    mode='offline'\n",
    ")\n",
    "train_file = 'DuReaderQG/train.json'\n",
    "valid_file = 'DuReaderQG/dev.json'\n",
    "model_checkpoint = \"./mengzi-t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a256ac-872b-4fc2-9363-1a2a66565e3f",
   "metadata": {},
   "source": [
    "### 创建dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bd5e347",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAdata(Dataset):\n",
    "    def __init__(self, file_name):\n",
    "        super().__init__()\n",
    "        self.data = self.load_data(file_name)\n",
    "\n",
    "    def load_data(self, file_name):\n",
    "        data = {}\n",
    "        with open(file_name, 'r') as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                data[idx] = json.loads(line)\n",
    "        return data\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "train_dataset = QAdata(train_file)\n",
    "valid_dataset = QAdata(valid_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1db854a-0387-44f2-b09c-0a5e941d696a",
   "metadata": {},
   "source": [
    "### 创建dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b235aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch_data):\n",
    "    contexts, questions, answers = [], [], []\n",
    "    for sample in batch_data:\n",
    "        contexts.append(sample['context'])\n",
    "        questions.append(sample['question'])\n",
    "        answers.append(sample['answer'])\n",
    "    inputs = tokenizer(\n",
    "        contexts,\n",
    "        questions,\n",
    "        text_target=answers,\n",
    "        max_length=256,\n",
    "        truncation='only_first',\n",
    "        padding=True,\n",
    "        stride=50,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    end_token_idx = torch.where(inputs['labels'] == tokenizer.eos_token_id)[1]\n",
    "    for idx, end_idx in enumerate(end_token_idx):\n",
    "        inputs['labels'][idx][end_idx + 1:] = -100 # 将无需预测的部分转换为-100，避免影响计算交叉熵损失\n",
    "    return inputs\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c65f65-dbf8-4220-9009-95bbd17247a1",
   "metadata": {},
   "source": [
    "### 训练和评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6965d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop(dataloader,  model, optimizer, lr_scheduler, total_loss, epoch):\n",
    "    process_bar = tqdm(range(len(dataloader)))\n",
    "    process_bar.set_description(f'Loss: {0:>7f}')\n",
    "    finished_batch_num = (epoch - 1) * len(dataloader)\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(dataloader, start=1):\n",
    "        batch = batch.to(device)\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        wandb.log({\"train/loss_step\": loss.item()}, step = finished_batch_num + idx)\n",
    "        total_loss += loss.item()\n",
    "        process_bar.set_description(f\"Loss : {total_loss / (finished_batch_num + idx):>7f}\")\n",
    "        process_bar.update(1)\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, bleu):\n",
    "    preds, labels = [], []\n",
    "    model.eval()\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            generations = model.generate(**batch).cpu().numpy()\n",
    "        decoded_generations = [' '.join(tokenizer.convert_ids_to_tokens(ids, skip_special_tokens=True)) for ids in generations]\n",
    "\n",
    "        label_ids = batch['labels'].cpu().numpy()\n",
    "        label_ids = np.where(label_ids!=-100, label_ids, tokenizer.pad_token_type_id)\n",
    "        decoded_labels = [' '.join(tokenizer.convert_ids_to_tokens(ids, skip_special_tokens=True)) for ids in label_ids]\n",
    "\n",
    "        preds += [pred.strip() for pred in decoded_generations]\n",
    "        labels += [[label.strip()] for label in decoded_labels]\n",
    "    return bleu.corpus_score(hypotheses=preds, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e58d6-15be-480d-bdd4-5ce440d7e878",
   "metadata": {},
   "source": [
    "### 训练主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e3c70d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3\n",
      "----------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4986bc9648f402988c72c4066fd180e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/908 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc93c8b6e9f0428aae7369b5a07a2412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | BLEU Score: 100.00\n",
      "BLEU-1: 100.00, BLEU-2: 100.00, BLEU-3: 100.00, BLEU-4: 100.00\n",
      "Epoch: 2/3\n",
      "----------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5f4915613a4535a01cba7724e59c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/908 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9ee90f3a8f42c0977a2ff58c17b6d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | BLEU Score: 100.00\n",
      "BLEU-1: 100.00, BLEU-2: 100.00, BLEU-3: 100.00, BLEU-4: 100.00\n",
      "Epoch: 3/3\n",
      "----------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10a923cccd643419f61d6ff241ac75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/908 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941ce653c6f54a44af5e129954ff82f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | BLEU Score: 100.00\n",
      "BLEU-1: 100.00, BLEU-2: 100.00, BLEU-3: 100.00, BLEU-4: 100.00\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "lr = 3e-5\n",
    "epoch_num = 3\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=epoch_num*len(train_dataloader)\n",
    ")\n",
    "total_loss = 0.\n",
    "bleu = BLEU()\n",
    "best_bleu = 0.\n",
    "model.to(device)\n",
    "for ep in range(epoch_num):\n",
    "    print(f\"Epoch: {ep + 1}/{epoch_num}\\n----------------\")\n",
    "    total_loss = train_loop(train_dataloader, model, optimizer, lr_scheduler, total_loss, ep + 1)\n",
    "    bleu_scores = test_loop(valid_dataloader, model, bleu) \n",
    "    wandb.log({\n",
    "        \"eval/bleu_score\": bleu_scores.score,\n",
    "            \"eval/bleu-1\": bleu_scores.precisions[0],\n",
    "            \"eval/bleu-2\": bleu_scores.precisions[1],\n",
    "            \"eval/bleu-3\": bleu_scores.precisions[2],\n",
    "            \"eval/bleu-4\": bleu_scores.precisions[3],\n",
    "            \"epoch\": ep\n",
    "    })\n",
    "    print(f\"Epoch {ep} | BLEU Score: {bleu_scores.score:.2f}\")\n",
    "    print(', '.join([f\"BLEU-{idx + 1}: {bleu_scores.precisions[idx]:.2f}\" for idx in range(4)]))\n",
    "    torch.save(\n",
    "        model.state_dict(), \n",
    "        f'epoch_{ep+1}_valid_bleu_{bleu_scores.score:0.2f}_model_weights.bin'\n",
    "    )\n",
    "print('Done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9199c39c-a87d-4e19-8847-f657a0432b0e",
   "metadata": {},
   "source": [
    "### 生成结果采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67129e37-0077-40e4-a098-bc3450ddd931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '年基准利率4.35%。 从实际看,贷款的基本条件是: 一是中国大陆居民,年龄在60岁以下; 二是有稳定的住址和工作或经营地点; 三是有稳定的收入来源; 四是无不良信用记录,贷款用途不能作为炒股,赌博等行为; 五是具有完全民事行为能力。2017年银行贷款基准利率', 'answer': '4.35%'}\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('epoch_3_valid_bleu_100.00_model_weights.bin', weights_only=False))\n",
    "results = []\n",
    "inputs, gens = [], []\n",
    "num_test = 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in valid_dataloader:\n",
    "        batch.to(device)\n",
    "        gen_ids = model.generate(**batch).cpu().numpy()\n",
    "        gen_tokens = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "        input_ids = batch['input_ids'].cpu().numpy()\n",
    "        input_tokens = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "        inputs += [s.strip() for s in input_tokens]\n",
    "        gens += [s.strip() for s in gen_tokens]\n",
    "        num_test -= 1\n",
    "        if num_test == 0:\n",
    "            break\n",
    "\n",
    "for q, a in zip(inputs, gens):\n",
    "    results.append({\n",
    "        \"question\": q,\n",
    "        \"answer\": a\n",
    "    })\n",
    "print(results[1])\n",
    "with open(\"preds.json\", \"wt\", encoding='utf-8') as f:\n",
    "    for example in results:\n",
    "        f.write(json.dumps(example, ensure_ascii=False) + '\\n')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
